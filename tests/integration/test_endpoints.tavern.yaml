---
test_name: Test Models API Endpoint
stages:
  - name: Test GET /api/v1/llama_stack/llms
    request:
      url: "{tavern.env_vars.TEST_BACKEND_URL}/api/v1/llama_stack/llms"
      method: GET
      headers:
        Accept: application/json
    response:
      status_code: 200
      headers:
        content-type: application/json
      json:
        - model_name: "ollama/llama3.2:1b-instruct-fp16"
          provider_resource_id: "llama3.2:1b-instruct-fp16"
          model_type: "llm"
---
test_name: Test Virtual Agents API Endpoint
stages:
  - name: Test POST /api/v1/virtual_agents
    request:
      url: "{tavern.env_vars.TEST_BACKEND_URL}/api/v1/virtual_agents/"
      method: POST
      headers:
        Accept: application/json
      json:
        name: "Test Assistant"
        prompt: "You are a helpful assistant."
        model_name: "ollama/llama3.2:1b-instruct-fp16"
        input_shields: []
        output_shields: []
        knowledge_base_ids: []
        tools: []
    response:
      status_code: 201
      headers:
        content-type: application/json
      save:
        json:
          agent_id: "id"
  - name: Test GET /api/v1/virtual_agents
    request:
      url: >-
        {tavern.env_vars.TEST_BACKEND_URL}/api/v1/virtual_agents/{agent_id}
      method: GET
      headers:
        Accept: application/json
    response:
      status_code: 200
      headers:
        content-type: application/json
      json:
        category: null
        template_id: null
        template_name: null
        suite_id: null
        suite_name: null
        sampling_strategy: null
        name: "Test Assistant"
        prompt: "You are a helpful assistant."
        model_name: "ollama/llama3.2:1b-instruct-fp16"
        input_shields: []
        output_shields: []
        temperature: null
        repetition_penalty: null
        max_tokens: null
        top_p: null
        top_k: null
        knowledge_base_ids: []
        vector_store_ids: []
        tools: []
        max_infer_iters: 100
        id: "{agent_id}"
        created_at: !anything
        updated_at: !anything
  - name: Test DELETE /api/v1/virtual_agents
    request:
      url: >-
        {tavern.env_vars.TEST_BACKEND_URL}/api/v1/virtual_agents/{agent_id}
      method: DELETE
      headers:
        Accept: application/json
    response:
      status_code: 204
---
test_name: Test LlamaStack Providers API
stages:
  - name: Test GET /api/v1/llama_stack/providers
    request:
      url: "{tavern.env_vars.TEST_BACKEND_URL}/api/v1/llama_stack/providers"
      method: GET
      headers:
        Accept: application/json
    response:
      status_code: 200
      json:
        - provider_id: "ollama"
          provider_type: "remote::ollama"
          config:
            base_url: "http://ollama:11434/v1"
          api: "inference"
        - provider_id: "llama-guard"
          provider_type: "inline::llama-guard"
          config:
            excluded_categories: []
          api: "safety"
        - provider_id: "meta-reference"
          provider_type: "inline::meta-reference"
          config:
            persistence:
              agent_state:
                namespace: "agents"
                backend: "kv_default"
              responses:
                table_name: "responses"
                backend: "sql_default"
                max_write_queue_size: 10000
                num_writers: 4
          api: "agents"
        - provider_id: "meta-reference-files"
          provider_type: "inline::localfs"
          config:
            storage_dir: "/.llama/distributions/ollama/files"
            metadata_store:
              table_name: "files_metadata"
              backend: "sql_default"
          api: "files"
        - provider_id: "model-context-protocol"
          provider_type: "remote::model-context-protocol"
          config: {}
          api: "tool_runtime"
