"""
LlamaStack Integration API endpoints for direct LlamaStack operations.

This module provides endpoints for interacting directly with LlamaStack
services, including LLM model management, chat interactions, and session
handling. It serves as the primary bridge between the frontend and
LlamaStack infrastructure.

Key Features:
- List available LLM models from LlamaStack
- Stream chat responses with session management
- Automatic session metadata storage for UI persistence
- Integration with virtual agent configurations
- Background task processing for database operations

The module handles real-time streaming chat interactions while maintaining
session state in both LlamaStack and the local database for UI features
like conversation history sidebars.
"""

import json
import logging
import os
import uuid
from typing import Any, Dict, List, Literal, Optional, Union

from fastapi import (
    APIRouter,
    Depends,
    HTTPException,
    Request,
    status,
)
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlalchemy import case, select
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.ext.asyncio import AsyncSession

from backend import models
from backend.database import get_db

from ..api.llamastack import get_client_from_request
from .chat import Chat

ATTACHMENTS_INTERNAL_API_ENDPOINT = os.getenv(
    "ATTACHMENTS_INTERNAL_API_ENDPOINT", "http://ai-virtual-agent:8000"
)


class TextContentItem(BaseModel):
    """
    Text content item for LlamaStack Responses API format.
    """

    type: Literal["input_text"]
    text: str


class ImageContentItem(BaseModel):
    """
    Image content item for LlamaStack Responses API format.
    """

    type: Literal["input_image"]
    image_url: str


# Union type for content items
ContentItem = Union[TextContentItem, ImageContentItem]


class Message(BaseModel):
    """
    Standard message format for LlamaStack chat interactions.

    Attributes:
        role: The role of the message sender ('user', 'assistant', or 'system')
        content: List of content items (text or images) in OpenAI/Responses API format
    """

    role: Literal["user", "assistant", "system"]
    content: List[ContentItem]


class VAChatMessage(BaseModel):
    """
    Extended chat message format for Virtual Agent interface compatibility.

    This format extends the basic Message format to include additional fields
    required by frontend chat interfaces and streaming implementations.

    Attributes:
        id: Optional unique identifier for the message
        role: The role of the message sender ('user', 'assistant', or 'system')
        content: The text content of the message
        parts: List of message parts for streaming/chunked content display
    """

    id: Optional[str] = None
    role: Literal["user", "assistant", "system"]
    content: str
    parts: List[str] = []  # Add parts field to match useChat format


logger = logging.getLogger(__name__)

router = APIRouter(prefix="/llama_stack", tags=["llama_stack"])


# Initialize LlamaStack client
@router.get("/llms", response_model=List[Dict[str, Any]])
async def get_llms(request: Request):
    """
    Retrieve all available Large Language Models from LlamaStack.

    Fetches the complete list of LLM models available in the LlamaStack
    instance, filtering for models with 'llm' API type and formatting
    them for frontend consumption. Used by chat interfaces to populate model
    selection dropdowns.

    Returns:
        List of dictionaries containing LLM model information:
        - model_name: The model identifier
        - provider_resource_id: Provider-specific resource identifier
        - model_type: API model type (always 'llm' for this endpoint)

    Raises:
        HTTPException: 502 if LlamaStack is unreachable, 500 for other errors
    """
    client = get_client_from_request(request)
    try:
        logger.info(f"Attempting to fetch models from LlamaStack at {client.base_url}")
        try:
            models = await client.models.list()
            logger.info(f"Received response from LlamaStack: {models}")
        except Exception as client_error:
            logger.error(f"Error calling LlamaStack API: {str(client_error)}")
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail=f"Failed to connect to LlamaStack API: {str(client_error)}",
            )

        if not models:
            logger.warning("No models returned from LlamaStack")
            return []

        llms = []
        for model in models:
            try:
                if model.api_model_type == "llm":
                    llm_config = {
                        "model_name": str(model.identifier),
                        "provider_resource_id": model.provider_resource_id,
                        "model_type": model.api_model_type,
                    }
                    llms.append(llm_config)
            except AttributeError as ae:
                logger.error(
                    f"Error processing model data: {str(ae)}. Model data: {model}"
                )
                continue

        logger.info(f"Successfully processed {len(llms)} LLM models")
        return llms

    except Exception as e:
        logger.error(f"Unexpected error in get_llms: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}",
        )


@router.get("/tools", response_model=List[Dict[str, Any]])
async def get_tools(request: Request):
    """
    Retrieve all available MCP (Model Context Protocol) servers from
    LlamaStack.

    Fetches tool groups that represent MCP servers configured in LlamaStack.
    These servers provide external tools and capabilities that can be used
    by virtual agents during conversations.

    Returns:
        List of dictionaries containing MCP server information:
        - id: The tool group identifier
        - name: Provider resource identifier
        - title: Provider identifier

    Raises:
        HTTPException: If LlamaStack communication fails
    """
    client = get_client_from_request(request)
    try:
        servers = await client.toolgroups.list()
        return [
            {
                "id": str(server.identifier),
                "name": server.provider_resource_id,
                "title": server.provider_id,
                "toolgroup_id": str(server.identifier),
            }
            for server in servers
        ]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/safety_models", response_model=List[Dict[str, Any]])
async def get_safety_models(request: Request):
    """
    Retrieve all available safety models from LlamaStack.

    Fetches models specifically designed for content safety and moderation.
    These models can be used to filter harmful content, detect inappropriate
    requests, and ensure safe AI interactions.

    Returns:
        List of dictionaries containing safety model information:
        - id: The model identifier
        - name: Provider resource identifier
        - model_type: Model type (always 'safety')

    Raises:
        HTTPException: If LlamaStack communication fails
    """
    client = get_client_from_request(request)
    try:
        models = await client.models.list()
        safety_models = []
        for model in models:
            if model.model_type == "safety":
                safety_model = {
                    "id": str(model.identifier),
                    "name": model.provider_resource_id,
                    "model_type": model.type,
                }
                safety_models.append(safety_model)
        return safety_models
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/embedding_models", response_model=List[Dict[str, Any]])
async def get_embedding_models(request: Request):
    """
    Retrieve all available embedding models from LlamaStack.

    Fetches models designed for generating vector embeddings from text.
    These models are used for knowledge base indexing, semantic search,
    and RAG (Retrieval Augmented Generation) operations.

    Returns:
        List of dictionaries containing embedding model information:
        - name: The model identifier
        - provider_resource_id: Provider-specific resource identifier
        - model_type: Model type (always 'embedding')

    Raises:
        HTTPException: If LlamaStack communication fails
    """
    client = get_client_from_request(request)
    try:
        models = await client.models.list()
        embedding_models = []
        for model in models:
            if model.model_type == "embedding":
                embedding_model = {
                    "name": str(model.identifier),
                    "provider_resource_id": model.provider_resource_id,
                    "model_type": model.type,
                }
                embedding_models.append(embedding_model)
        return embedding_models
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/shields", response_model=List[Dict[str, Any]])
async def get_shields(request: Request):
    """
    Retrieve all available safety shields from LlamaStack.

    Fetches shield configurations used for content filtering and safety
    enforcement. Shields provide pre-configured safety policies that can
    be applied to AI interactions to prevent harmful content generation.

    Returns:
        List of dictionaries containing shield information:
        - id: The shield identifier
        - name: Provider resource identifier
        - model_type: Shield type identifier

    Raises:
        HTTPException: If LlamaStack communication fails
    """
    client = get_client_from_request(request)
    try:
        shields = await client.shields.list()
        shields_list = []
        for shield in shields:
            shield = {
                "id": str(shield.identifier),
                "name": shield.provider_resource_id,
                "model_type": shield.type,
            }
            shields_list.append(shield)
        return shields_list
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/providers", response_model=List[Dict[str, Any]])
async def get_providers(request: Request):
    """
    Retrieve all available providers from LlamaStack.

    Fetches the complete list of providers configured in LlamaStack,
    including their configurations and supported APIs. Providers are
    the underlying services that supply models, tools, and other capabilities.

    Returns:
        List of dictionaries containing provider information:
        - provider_id: The provider identifier
        - provider_type: Type of provider (e.g., 'meta-reference', 'openai')
        - config: Provider-specific configuration parameters
        - api: List of APIs supported by this provider

    Raises:
        HTTPException: If LlamaStack communication fails
    """
    client = get_client_from_request(request)
    try:
        providers = await client.providers.list()
        return [
            {
                "provider_id": str(provider.provider_id),
                "provider_type": provider.provider_type,
                "config": (provider.config if hasattr(provider, "config") else {}),
                "api": provider.api,
            }
            for provider in providers
        ]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


class ChatRequest(BaseModel):
    """
    Request model for LlamaStack chat interactions.

    Defines the structure for chat requests sent to the LlamaStack chat
    endpoint. Includes virtual agent selection, message content,
    streaming preferences, and session management.

    Attributes:
        virtualAgentId: The ID of the virtual agent to use for chat
        message: Single message with content items (text, images, etc.)
        stream: Whether to stream the response (default: False)
        sessionId: Optional session ID for UI purposes
    """

    virtualAgentId: str
    message: Message
    stream: bool = False
    sessionId: Optional[str] = None


@router.post("/chat")
async def chat(
    chatRequest: ChatRequest,
    request: Request,
    db: AsyncSession = Depends(get_db),
):
    """
    Main chat endpoint for streaming conversations with LlamaStack agents.

    Handles real-time chat interactions by streaming responses from LlamaStack
    agents while maintaining session state. Automatically saves session
    metadata to the database for UI features like conversation history
    sidebars and handles response chaining by looking up previous response IDs.

    The endpoint validates the virtual agent exists, looks up the previous
    response ID from the database for conversation chaining, and streams
    responses in Server-Sent Events format. Session metadata is saved
    synchronously after the response completes.

    Args:
        chatRequest: ChatRequest containing agent ID, message, and session info
        request: FastAPI request object for LlamaStack client access
        db: Database session for metadata operations and response ID lookup

    Returns:
        StreamingResponse: Server-Sent Events stream of chat responses

    Raises:
        HTTPException:
            - 404 if virtual agent not found in database
            - 500 for internal server errors during chat processing
    """
    try:
        logger.info(f"Received chatRequest: {chatRequest.model_dump()}")

        # Get the agent config from our local database instead of LlamaStack
        try:
            result = await db.execute(
                select(models.VirtualAgentConfig).where(
                    models.VirtualAgentConfig.id == chatRequest.virtualAgentId
                )
            )
            agent_config = result.scalar_one_or_none()
            if not agent_config:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Virtual agent {chatRequest.virtualAgentId} not found",
                )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Error retrieving agent config: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Error retrieving agent configuration: {str(e)}",
            )

        # Use the agent_id from our database
        agent_id = chatRequest.virtualAgentId

        # Session ID for UI purposes - generate one if not provided
        session_id = chatRequest.sessionId
        if not session_id:
            session_id = str(uuid.uuid4())
            logger.info(f"Generated new session ID: {session_id}")

        # Create responses-based Chat instance with database access
        chat = Chat(request, db)

        async def chat_stream():
            response_id = None
            try:
                logger.info(
                    f"About to call chat.stream with agent_id: {agent_id}, "
                    f"session_id: {session_id}"
                )
                async for chunk in chat.stream(
                    agent_id,
                    session_id,
                    chatRequest.message.content,
                ):
                    logger.info(f"Received chunk from chat.stream: {chunk}")

                    # Extract response_id from chunk if available
                    try:
                        chunk_data = json.loads(chunk)
                        if chunk_data.get("response_id"):
                            response_id = chunk_data["response_id"]
                    except (json.JSONDecodeError, KeyError, TypeError):
                        pass  # Not all chunks contain JSON or response_id

                    yield f"data: {chunk}\n\n"

                yield "data: [DONE]\n\n"

                # Save session metadata to database with response_id (synchronously)
                await save_session_metadata(
                    db,
                    session_id,
                    agent_id,
                    chatRequest.message,
                    request,
                    response_id,
                )

            except Exception as e:
                logger.error(f"Error in stream: {str(e)}")
                yield (f'data: {{"type":"error","content":"Error: {str(e)}"}}\n\n')

        return StreamingResponse(chat_stream(), media_type="text/event-stream")

    except Exception as e:
        logger.error(f"Error in chat endpoint: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e)
        )


async def save_session_metadata(
    db: AsyncSession,
    session_id: str,
    agent_id: str,
    message: Message,
    request: Request,
    response_id: Optional[str] = None,
):
    """
    Save session metadata to database for UI sidebar display.

    Asynchronously saves or updates session information in the database to
    support frontend features like conversation history sidebars. Generates
    meaningful session titles from the first user message and fetches agent
    display names.

    This function is called as a background task to avoid blocking chat
    responses while ensuring session metadata is available for the UI.

    Args:
        db: Database session for executing queries
        session_id: Unique identifier for the chat session
        agent_id: ID of the virtual agent used in the session
        messages: List of conversation messages for title generation

    Note:
        Errors in metadata saving are logged but don't affect chat
        functionality.
        Uses INSERT ... ON CONFLICT DO UPDATE for idempotent session storage.
    """
    try:
        # Generate title from user message
        title = "New Chat"
        if message and message.role == "user":
            txt = ""
            # Extract text from message content
            for item in message.content:
                if item.type == "input_text":
                    txt = item.text
                    break
            if txt:
                title = txt[:50] + "..." if len(txt) > 50 else txt[:50]

        # Insert or update session metadata
        stmt = insert(models.ChatSession).values(
            id=session_id,
            title=title,
            agent_id=agent_id,
            session_state={
                "last_response_id": response_id,
            },
        )
        stmt = stmt.on_conflict_do_update(
            index_elements=["id"],
            set_=dict(
                # Only update title if current title is a generated/default title
                title=case(
                    (
                        models.ChatSession.title.in_(["New Chat", "Chat", ""])
                        | models.ChatSession.title.like("Chat-%"),
                        stmt.excluded.title,
                    ),
                    else_=models.ChatSession.title,
                ),
                agent_id=stmt.excluded.agent_id,
                updated_at=stmt.excluded.updated_at,
                session_state=stmt.excluded.session_state,
            ),
        )

        await db.execute(stmt)
        await db.commit()
        logger.info(f"Saved session metadata for {session_id}")

    except Exception as e:
        logger.error(f"Error saving session metadata: {e}")
        await db.rollback()
